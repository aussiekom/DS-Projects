{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection\n",
        "\n",
        "When building a predictive model, we often have many features or variable in our dataset that can be used to train our model. However, just because the feature exists in our dataset does not mean that it is relevant for our model or that we should use it.\n",
        "\n",
        "How do we know which features to use in our model?\n",
        "\n",
        "This is where feature selection comes in. Feature selection is simply a process that reduces the number of input variables, in order to keep only the most important ones.\n",
        "\n",
        "There is an advantage in reducing the number of input features, as it simplifies the model, reduces the computation cost, and it can also improve the modelâ€™s performance.\n",
        "\n",
        "Now, how do we decide which feature is important? What does it mean for a feature to be important?\n",
        "\n",
        "There is no clear answer for that, so we need to experiment with different methods and see which gives the best results.\n",
        "\n",
        "In this article, we are going to explore and implement three different feature selection methods:\n",
        "\n",
        "* variance threshold\n",
        "* K best features\n",
        "* recursive feature elimination (RFE)"
      ],
      "metadata": {
        "id": "uM6JVR9KVysM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data\n",
        "\n",
        "We will use the wine dataset available on sklearn.\n",
        "\n",
        "The dataset contains 178 rows with 13 features and a target containing three unique categories. This is therefore a classification task."
      ],
      "metadata": {
        "id": "4OP_RO5-XFse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J2rgflnrVuw5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine_data = load_wine()\n",
        "\n",
        "wine_df = pd.DataFrame(\n",
        "    data=wine_data.data,\n",
        "    columns=wine_data.feature_names)\n",
        "\n",
        "wine_df['target'] = wine_data.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wine_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "ivNhPgJsXR9s",
        "outputId": "701284d2-2442-4325-9755-c88d9e407dc9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
              "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
              "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
              "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
              "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
              "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
              "..       ...         ...   ...                ...        ...            ...   \n",
              "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
              "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
              "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
              "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
              "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
              "\n",
              "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
              "0          3.06                  0.28             2.29             5.64  1.04   \n",
              "1          2.76                  0.26             1.28             4.38  1.05   \n",
              "2          3.24                  0.30             2.81             5.68  1.03   \n",
              "3          3.49                  0.24             2.18             7.80  0.86   \n",
              "4          2.69                  0.39             1.82             4.32  1.04   \n",
              "..          ...                   ...              ...              ...   ...   \n",
              "173        0.61                  0.52             1.06             7.70  0.64   \n",
              "174        0.75                  0.43             1.41             7.30  0.70   \n",
              "175        0.69                  0.43             1.35            10.20  0.59   \n",
              "176        0.68                  0.53             1.46             9.30  0.60   \n",
              "177        0.76                  0.56             1.35             9.20  0.61   \n",
              "\n",
              "     od280/od315_of_diluted_wines  proline  target  \n",
              "0                            3.92   1065.0       0  \n",
              "1                            3.40   1050.0       0  \n",
              "2                            3.17   1185.0       0  \n",
              "3                            3.45   1480.0       0  \n",
              "4                            2.93    735.0       0  \n",
              "..                            ...      ...     ...  \n",
              "173                          1.74    740.0       2  \n",
              "174                          1.56    750.0       2  \n",
              "175                          1.56    835.0       2  \n",
              "176                          1.62    840.0       2  \n",
              "177                          1.60    560.0       2  \n",
              "\n",
              "[178 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e99259e-a09d-4fde-824a-08b1234ecd2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows Ã— 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e99259e-a09d-4fde-824a-08b1234ecd2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e99259e-a09d-4fde-824a-08b1234ecd2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e99259e-a09d-4fde-824a-08b1234ecd2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-349fcd4c-eab6-4364-909b-149b85ebf3a9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-349fcd4c-eab6-4364-909b-149b85ebf3a9')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-349fcd4c-eab6-4364-909b-149b85ebf3a9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the data\n",
        "Before implementing feature selection techniques, we first split our data into a training and test set.\n",
        "\n",
        "That way, we have fixed starting points and a fixed test set so that we can compare the impact of each feature selection method on the modelâ€™s performance."
      ],
      "metadata": {
        "id": "T1f30_KHXcfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = wine_df.drop(['target'], axis=1)\n",
        "y = wine_df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "-b42J17_XcOc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this split, we have 124 samples for the training set, and 54 samples for the test set. Note the use of stratify, which ensures that both the train and test sets contain the same proportion of the target class."
      ],
      "metadata": {
        "id": "CXXZfjDzXiBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting the best features\n",
        "We now dive into the feature selection methods. As mentioned above we will try three different methods and see how it impacts the modelâ€™s performance.\n",
        "\n",
        "To make this experiment robust, we will use a simple decision tree classifier."
      ],
      "metadata": {
        "id": "JP3qY0AFXivW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variance threshold\n",
        "The first method we will explore is the variance threshold. This is, of course, based on the variance, which is a measure of dispersion. In other words, **it measures how far a set of number is spread out from their average value.**\n",
        "\n",
        "For example, the variance of [1, 1, 1, 1, 1] is 0, because each number is equal to their average value. Therefore, they do not spread out from their mean value.\n",
        "\n",
        "Variance threshold then simply removes any feature with a variance that is below a given threshold.\n",
        "\n",
        "We can see how this is useful to remove features with a variance close to 0, because this means that the values are constant or vary only slightly across all samples of the dataset. Therefore, they do not have any predictive power.\n",
        "\n",
        "Thus, letâ€™s compare the variance of each feature in our training set."
      ],
      "metadata": {
        "id": "OR6ToU8PX8Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_v1 = X_train.copy()\n",
        "X_train_v1.var(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZuIS8jMYFKw",
        "outputId": "0cd9f951-4446-4aa5-e7cb-6045b2deec00"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alcohol                             0.658341\n",
              "malic_acid                          1.123507\n",
              "ash                                 0.072433\n",
              "alcalinity_of_ash                  11.471279\n",
              "magnesium                         232.071532\n",
              "total_phenols                       0.393226\n",
              "flavanoids                          0.912299\n",
              "nonflavanoid_phenols                0.013873\n",
              "proanthocyanins                     0.335108\n",
              "color_intensity                     5.669722\n",
              "hue                                 0.052891\n",
              "od280/od315_of_diluted_wines        0.470021\n",
              "proline                         94906.710923\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can see that the variance for each of the features of the training set. However, we cannot define a variance threshold just yet, because our data does not have the same scale, and so the variance is not on the same scale either.\n",
        "\n",
        "Data on a larger scale can have a higher variance than features on a smaller scale, even if their distribution is similar.\n",
        "\n",
        "Thus, it is important to first scale our data before defining a threshold. Here, we normalize the data and then calculate the variance."
      ],
      "metadata": {
        "id": "-MkKPdrzYEwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "norm = Normalizer().fit(X_train_v1)\n",
        "norm_X_train = norm.transform(X_train_v1)\n",
        "norm_X_train.var(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr7VNf1mYPxY",
        "outputId": "5c44b437-6b25-4c2c-e2a9-9787b0cb1b3d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.91904873e-05, 6.21699082e-06, 1.84444433e-06, 2.09448705e-04,\n",
              "       2.68754975e-03, 2.30121336e-06, 2.94585256e-06, 9.07454007e-08,\n",
              "       1.52283266e-06, 1.56043625e-05, 5.25705889e-07, 3.50720354e-06,\n",
              "       8.83697816e-05])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see that the variance of each scaled feature. While the variance is very small for all, some feature have incredibly small variance, with power to -8 and to -7.\n",
        "\n",
        "Thus, letâ€™s set our threshold to 1e-6. Any feature with a variance below that threshold will be removed."
      ],
      "metadata": {
        "id": "rv1wGhFwYTVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold = 1e-6)\n",
        "selected_features = selector.fit_transform(norm_X_train)\n",
        "selected_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xHjb1xqYSTa",
        "outputId": "600f88b7-b593-4be9-c24b-4aaeebe1587c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see how a decision tree classifier performs when using all the features available, and we remove the two features mentioned above."
      ],
      "metadata": {
        "id": "_YykE5nnYZjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "#Classifier with all features\n",
        "dt.fit(X_train, y_train)\n",
        "preds = dt.predict(X_test)\n",
        "f1_score_all = round(f1_score(y_test, preds, average='weighted'),3)\n",
        "\n",
        "# Classifier with selected features with variance threshold\n",
        "X_train_sel = X_train.drop(['hue', 'nonflavanoid_phenols'], axis=1)\n",
        "X_test_sel = X_test.drop(['hue', 'nonflavanoid_phenols'], axis=1)\n",
        "\n",
        "dt.fit(X_train_sel, y_train)\n",
        "\n",
        "preds_sel = dt.predict(X_test_sel)\n",
        "f1_score_sel = round(f1_score(y_test, preds_sel, average='weighted'), 3)"
      ],
      "metadata": {
        "id": "5x970J_qYaVt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the F1-score for both models gives 0.963. Thus, removing two features did not improve the model, but we do achieve the same performance.\n",
        "\n",
        "Therefore, a simpler model with fewer features achieved the same result as using all features, which is a good sign.\n",
        "\n",
        "Now, using variance threshold is somewhat simple, and setting the threshold is somewhat arbitrary.\n",
        "\n",
        "With the next method, we can easily find the optimal number of variables to keep while deciding the selection criteria."
      ],
      "metadata": {
        "id": "dwlHPZPkYhtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K best features\n",
        "Here, we use a method that gives more flexibility in evaluating the importance of a feature.\n",
        "\n",
        "The algorithm is simple: we simply provide a method of calculating the importance of a feature and the number of features we want to use, denoted as k. Then, the algorithm simply returns the top k features.\n",
        "\n",
        "The main advantage of this method is that we are free to choose among a variety of ways to compute the importance of a feature. For example, we can use the chi squared test to quantify the independence of a feature to a the target. The higher the score, the higher the dependency between the feature and the target, and so the higher the importance of that feature.\n",
        "\n",
        "Other methods can be used, such as computing the mutual information, using the False Positive Rate test, or calculating the F-statistic for a regression task.\n",
        "\n",
        "Now, we still have the challenge of determining how many variables should be selected for the model. Here, since we are working with only 13 features in total, letâ€™s try using one to all features and see which configuration gives us the best results.\n",
        "\n",
        "Here, we use the chi squared test, as we are working with a classification task."
      ],
      "metadata": {
        "id": "jQYVOIx5Yi7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "X_train_v2, X_test_v2, y_train_v2, y_test_v2 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
        "f1_score_list = []\n",
        "\n",
        "for k in range(1, 14):\n",
        "    selector = SelectKBest(chi2, k=k)\n",
        "    selector.fit(X_train_v2, y_train_v2)\n",
        "\n",
        "    sel_X_train_v2 = selector.transform(X_train_v2)\n",
        "    sel_X_test_v2 = selector.transform(X_test_v2)\n",
        "\n",
        "    dt.fit(sel_X_train_v2, y_train_v2)\n",
        "    kbest_preds = dt.predict(sel_X_test_v2)\n",
        "\n",
        "f1_score_kbest = round(f1_score(y_test, kbest_preds, average='weighted'), 3)\n",
        "f1_score_list.append(f1_score_kbest)\n",
        "\n",
        "print(f1_score_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn0ZQrbqYm3H",
        "outputId": "de7c11c0-6add-4533-cc98-4c0931dac79a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.963]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now plot the F1-score for each number of variables used in the model:"
      ],
      "metadata": {
        "id": "QA2JmQjMYv33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = ['1','2','3','4','5','6','7','8','9','10','11','12','13']\n",
        "y = f1_score_list\n",
        "\n",
        "ax.bar(x, y, width=0.4)\n",
        "ax.set_xlabel('Number of features (selected using chi2 test)')\n",
        "ax.set_ylabel('F1-Score (weighted)')\n",
        "ax.set_ylim(0, 1.2)\n",
        "\n",
        "for index, value in enumerate(y):\n",
        "    plt.text(x=index, y=value + 0.05, s=str(value), ha='center')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "nqNl3-l2YvqE",
        "outputId": "e9d1d612-3ab6-4d0b-f249-c0ef772f2040"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the figure above, we see that using only the most important feature, according to the chi squared test, gives us the worst performance, while using the top 11 features results in a perfect F1-score. However, letâ€™s also notice that using the top 6 features gives an F1-score of 0.981, which is a small drawback considering that we are using half of the features. Plus, we see a similar performance using only the top 4 features.\n",
        "\n",
        "With this example, we can clearly see how feature selection can only simplify the model, but also increase its performance, even if we are working with a simple toy dataset.\n",
        "\n",
        "Of course, using a different evaluator of the feature importance might lead to different results. I invite you to use another test than the chi squared test, and see for yourself if the resulting plot changes drastically.\n",
        "\n",
        "Letâ€™s move on to the last method we will implement: Recursive Feature Elimination or RFE."
      ],
      "metadata": {
        "id": "B0aUqyuFZPKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Feature Elimination (RFE)\n",
        "\n",
        "The last method we implement in this article is Recursive Feature Elimination or RFE."
      ],
      "metadata": {
        "id": "hryAwks_ZRHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is interesting about this feature selection method, is that it relies on the modelâ€™s capacity to evaluate the importance of a feature. Therefore, we must use a model that returns either a coefficient or a measure of feature importance.\n",
        "\n",
        "Here, since we are using a decision tree, the model can actually calculate the importance of a feature.\n",
        "\n",
        "In a decision tree, the importance of a feature is calculated as the decrease in node impurity multiplied by the probability of reaching that node. This is based on the CART algorithm that runs behind the scenes of a decision tree.\n",
        "\n",
        "Now, letâ€™s test the RFE selection method. We will set the number of input variables to 4, and see how the performance compares to selecting the top 4 features using the chi squared test. That way, we will see which method of evaluating feature importance (chi squared test or feature importance as computed by the decision tree) can choose the best four features for our model.\n",
        "\n",
        "Implementing RFE is very straightforward using sklearn. First, we define the selector and specify the number of features we want. In this case, we want four. Then, we fit the selector on the training set, which will return us the top 4 features.\n",
        "\n"
      ],
      "metadata": {
        "id": "Beiw5AzUZVfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
        "\n",
        "RFE_selector = RFE(estimator=dt, n_features_to_select=4, step=1)\n",
        "\n",
        "RFE_selector.fit(X_train_v3, y_train_v3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "XZRS3bS9ZZEg",
        "outputId": "adfb4693-0e3d-4667-9c69-538a8a2c591c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=4)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFE</label><div class=\"sk-toggleable__content\"><pre>RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see the name of the selected features by accessing the support_ attribute."
      ],
      "metadata": {
        "id": "oo2HyRjbZdp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_v3.columns[RFE_selector.support_]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y6m7yV2Ze2Z",
        "outputId": "2dd0a7c8-dfd0-435b-eef9-f560a7ff7478"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['alcalinity_of_ash', 'flavanoids', 'color_intensity', 'proline'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we see that the top 4 features are:\n",
        "\n",
        "* alcalinity_of_ash\n",
        "* flavanoids\n",
        "* color_intensity\n",
        "* proline"
      ],
      "metadata": {
        "id": "dP7ICwaaZjz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can fit the model using only those top 4 features and calculate the F1-score on the test set."
      ],
      "metadata": {
        "id": "FqKK_YpBZpkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel_X_train_v3 = RFE_selector.transform(X_train_v3)\n",
        "sel_X_test_v3 = RFE_selector.transform(X_test_v3)\n",
        "\n",
        "dt.fit(sel_X_train_v3, y_train_v3)\n",
        "\n",
        "RFE_preds = dt.predict(sel_X_test_v3)\n",
        "\n",
        "rfe_f1_score = round(f1_score(y_test_v3, RFE_preds, average='weighted'),3)\n",
        "print(rfe_f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvpfvsQMZqjH",
        "outputId": "c9ffb741-6974-4614-a456-c167f3faa682"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "We explored and implemented three different feature selection methods:\n",
        "\n",
        "* variance threshold\n",
        "* K best features\n",
        "* recursive feature elimination (RFE)\n",
        "\n",
        "Variance threshold is good to remove features that have 0 variance, as a constant variable is definitely not a good variable. However, setting a variance threshold is hard and quite arbitrary, and I suggest using it along K best features or RFE.\n",
        "\n",
        "With K best features, we are able to choose how to evaluate the importance of a feature, which also allows us to determine the best method and the best number of features to include in our model.\n",
        "\n",
        "Finally, RFE is another feature selection method, that relies on the model itself to compute the importance of a feature."
      ],
      "metadata": {
        "id": "bqji9sQuZzPW"
      }
    }
  ]
}